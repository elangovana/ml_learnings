{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a tutorial to lean how LDA works under the hood, based on the tutorial by andrew woods, http://brooksandrew.github.io/simpleblog/articles/latent-dirichlet-allocation-under-the-hood/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PARAMETERS\n",
    "K = 2 # number of topics\n",
    "alpha = 1 # hyperparameter. single value indicates symmetric dirichlet prior. higher=>scatters document clusters\n",
    "eta = .001 # hyperparameter\n",
    "iterations = 3 # iterations for collapsed gibbs sampling.  This should be a lot higher than 3 in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawdocs = ['eat turkey on turkey day holiday',\n",
    "          'i like to eat cake on holiday',\n",
    "          'turkey trot race on thanksgiving holiday',\n",
    "          'snail race the turtle',\n",
    "          'time travel space race',\n",
    "          'movie on thanksgiving',\n",
    "          'movie at air and space museum is cool movie',\n",
    "          'aspiring movie star']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert raw docs into bag of words docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_bow = [ d.split(' ') for d in rawdocs]\n",
    "print(docs_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct vocabulary by obtaining unique words from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocabulary = np.unique( reduce( operator.concat, docs_bow) ).tolist()\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the words in documents with the index from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for d in  docs_bow:\n",
    "    docs.append([vocabulary.index(w) for w in d])\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a topic to each word in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import random\n",
    "\n",
    "topic_word_count = np.zeros(shape=(K, len(vocabulary)), dtype=np.int8).tolist()\n",
    "dword_topic_assign = [np.arange(len(d)).tolist() for d in docs]\n",
    "\n",
    "for d in range(0, len(docs)):\n",
    "    for w in range(0, len(docs[d])):\n",
    "        ##Randomly assign a topic to a word in the document\n",
    "        dword_topic_assign [d][w] = random.randint(1, K)\n",
    "        ti = dword_topic_assign [d][w] - 1 #topic index\n",
    "        wi = docs[d][w] # word index\n",
    "        topic_word_count[ti][wi] = topic_word_count[ti][wi] + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_count = []\n",
    "for d in range(0, len(docs)):\n",
    "    \n",
    "    doc_topic_count.append([])    \n",
    "    for t in range(1, K+1):\n",
    "        total = len( [ [ta] for ta in dword_topic_assign[d] if ta == t ])\n",
    "        doc_topic_count[d].append(total)\n",
    "\n",
    "print(doc_topic_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, iterations+1):\n",
    "    for d in range(0, len(docs)):\n",
    "        for w in range(0, len(docs[d])):\n",
    "            current_topic_assignment =  dword_topic_assign[d][w] \n",
    "            tid = current_topic_assignment -1\n",
    "            wid = docs[d][w]\n",
    "            \n",
    "           \n",
    "            #remove weight, essentially remove all topic linked to that word before ressigning to \n",
    "            doc_topic_count[d][tid]= doc_topic_count[d][tid]-1\n",
    "            topic_word_count[tid][ wid]= topic_word_count[tid][ wid] -1\n",
    "            \n",
    "            denom_a_doc_word_count_based = sum(doc_topic_count[d])  + K * alpha ## length of the document\n",
    "            denom_b_per_topic_word_count_based = [ sum(t) + len(vocabulary) * eta for t in topic_word_count]   \n",
    "            \n",
    "   \n",
    "            print(np.array(topic_word_count)[:,wid])\n",
    "            print((np.array(topic_word_count)[:,wid] + eta)/ denom_b_per_topic_word_count_based)\n",
    "            print((np.array(doc_topic_count[d]) + alpha)/ denom_a_doc_word_count_based) \n",
    "         \n",
    "            \n",
    "            \n",
    "            p_z = (np.array(topic_word_count)[:,wid] + eta)/ denom_b_per_topic_word_count_based * (np.array(doc_topic_count[d]) + alpha) / denom_a_doc_word_count_based\n",
    "            \n",
    "            print( p_z)\n",
    "            print( p_z/sum(p_z))\n",
    "            new_topic_assignment = np.random.choice(range(1, K+1), 1, (p_z/sum(p_z)).tolist())[0]\n",
    "            \n",
    "            \n",
    "            \n",
    "            #Assign the new topic\n",
    "            ntid = new_topic_assignment -1\n",
    "            dword_topic_assign[d][w]  = new_topic_assignment\n",
    "       \n",
    "            doc_topic_count[d][ntid]=doc_topic_count[d][ntid]+1\n",
    "            topic_word_count[ntid][ wid] = topic_word_count[ntid][ wid] +1\n",
    "            \n",
    "            if(current_topic_assignment != new_topic_assignment):\n",
    "                print(\"doc: {}  token: {}  topic from {} to topic {}\".format(d, vocabulary[wid], current_topic_assignment, new_topic_assignment )) # examine when topic assignments change\n",
    " \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
