{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a tutorial to lean how LDA works under the hood, based on the tutorial by andrew woods, http://brooksandrew.github.io/simpleblog/articles/latent-dirichlet-allocation-under-the-hood/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdocs = ['eat turkey on turkey day holiday',\n",
    "          'i like to eat cake on holiday',\n",
    "          'turkey trot race on thanksgiving holiday',\n",
    "          'snail race the turtle',\n",
    "          'time travel space race',\n",
    "          'movie on thanksgiving',\n",
    "          'movie at air and space museum is cool movie',\n",
    "          'aspiring movie star']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert raw docs into bag of words docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['eat', 'turkey', 'on', 'turkey', 'day', 'holiday'], ['i', 'like', 'to', 'eat', 'cake', 'on', 'holiday'], ['turkey', 'trot', 'race', 'on', 'thanksgiving', 'holiday'], ['snail', 'race', 'the', 'turtle'], ['time', 'travel', 'space', 'race'], ['movie', 'on', 'thanksgiving'], ['movie', 'at', 'air', 'and', 'space', 'museum', 'is', 'cool', 'movie'], ['aspiring', 'movie', 'star']]\n"
     ]
    }
   ],
   "source": [
    "docs_bow = [ d.split(' ') for d in rawdocs]\n",
    "print(docs_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct vocabulary by obtaining unique words from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['air', 'and', 'aspiring', 'at', 'cake', 'cool', 'day', 'eat', 'holiday', 'i', 'is', 'like', 'movie', 'museum', 'on', 'race', 'snail', 'space', 'star', 'thanksgiving', 'the', 'time', 'to', 'travel', 'trot', 'turkey', 'turtle']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocabulary = np.unique( reduce( operator.concat, docs_bow) ).tolist()\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the words in documents with the index from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7, 25, 14, 25, 6, 8], [9, 11, 22, 7, 4, 14, 8], [25, 24, 15, 14, 19, 8], [16, 15, 20, 26], [21, 23, 17, 15], [12, 14, 19], [12, 3, 0, 1, 17, 13, 10, 5, 12], [2, 12, 18]]\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for d in  docs_bow:\n",
    "    docs.append([vocabulary.index(w) for w in d])\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
