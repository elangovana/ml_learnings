{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a tutorial to lean how LDA works under the hood, based on the tutorial by andrew woods, http://brooksandrew.github.io/simpleblog/articles/latent-dirichlet-allocation-under-the-hood/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PARAMETERS\n",
    "K = 2 # number of topics\n",
    "alpha = 1 # hyperparameter. single value indicates symmetric dirichlet prior. higher=>scatters document clusters\n",
    "eta = .001 # hyperparameter\n",
    "iterations = 3 # iterations for collapsed gibbs sampling.  This should be a lot higher than 3 in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawdocs = ['eat turkey on turkey day holiday',\n",
    "          'i like to eat cake on holiday',\n",
    "          'turkey trot race on thanksgiving holiday',\n",
    "          'snail race the turtle',\n",
    "          'time travel space race',\n",
    "          'movie on thanksgiving',\n",
    "          'movie at air and space museum is cool movie',\n",
    "          'aspiring movie star']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert raw docs into bag of words docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['eat', 'turkey', 'on', 'turkey', 'day', 'holiday'], ['i', 'like', 'to', 'eat', 'cake', 'on', 'holiday'], ['turkey', 'trot', 'race', 'on', 'thanksgiving', 'holiday'], ['snail', 'race', 'the', 'turtle'], ['time', 'travel', 'space', 'race'], ['movie', 'on', 'thanksgiving'], ['movie', 'at', 'air', 'and', 'space', 'museum', 'is', 'cool', 'movie'], ['aspiring', 'movie', 'star']]\n"
     ]
    }
   ],
   "source": [
    "docs_bow = [ d.split(' ') for d in rawdocs]\n",
    "print(docs_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct vocabulary by obtaining unique words from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['air', 'and', 'aspiring', 'at', 'cake', 'cool', 'day', 'eat', 'holiday', 'i', 'is', 'like', 'movie', 'museum', 'on', 'race', 'snail', 'space', 'star', 'thanksgiving', 'the', 'time', 'to', 'travel', 'trot', 'turkey', 'turtle']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocabulary = np.unique( reduce( operator.concat, docs_bow) ).tolist()\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the words in documents with the index from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7, 25, 14, 25, 6, 8], [9, 11, 22, 7, 4, 14, 8], [25, 24, 15, 14, 19, 8], [16, 15, 20, 26], [21, 23, 17, 15], [12, 14, 19], [12, 3, 0, 1, 17, 13, 10, 5, 12], [2, 12, 18]]\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for d in  docs_bow:\n",
    "    docs.append([vocabulary.index(w) for w in d])\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 3, 1, 2, 2, 1, 0, 1, 1, 1, 0,\n        0, 0, 0, 2, 1],\n       [1, 1, 0, 1, 1, 0, 0, 1, 2, 1, 0, 1, 1, 0, 2, 1, 0, 2, 0, 1, 0, 1,\n        1, 1, 1, 1, 0]])\n[[2, 2, 2, 1, 1, 2],\n [2, 2, 2, 1, 2, 1, 2],\n [1, 2, 1, 2, 2, 1],\n [1, 1, 1, 1],\n [2, 2, 2, 2],\n [1, 1, 1],\n [1, 2, 2, 2, 2, 1, 1, 1, 1],\n [1, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import random\n",
    "\n",
    "tw = np.zeros(shape=(K, len(vocabulary)), dtype=np.int8).tolist()\n",
    "dt = [np.arange(len(d)).tolist() for d in docs]\n",
    "\n",
    "for d in range(0, len(docs)):\n",
    "    for w in range(0, len(docs[d])):\n",
    "        ##Randomly assign a topic to a word in the document\n",
    "        dt[d][w] = random.randint(1, K)\n",
    "        ti = dt[d][w] - 1 #topic index\n",
    "        wi = docs[d][w] # word index\n",
    "        tw[ti][wi] = tw[ti][wi] + 1\n",
    "\n",
    "pprint.pprint(np.array(tw))\n",
    "pprint.pprint(dt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
